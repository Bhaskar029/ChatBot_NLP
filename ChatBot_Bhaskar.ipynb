{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10b6ae0",
   "metadata": {},
   "source": [
    "**CHAT BOT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c6488",
   "metadata": {},
   "source": [
    "A. Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5530451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15be4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('ChatBot_Corpus.json') as file: #added some inputs from myself in the Corpus. Eg. links, diff patterns\n",
    "    Corpus=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0e4e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'Intro',\n",
       "   'patterns': ['hi',\n",
       "    'how are you',\n",
       "    'is anyone there',\n",
       "    'hello',\n",
       "    'whats up',\n",
       "    'hey',\n",
       "    'yo',\n",
       "    'listen',\n",
       "    'please help me',\n",
       "    'i am learner from',\n",
       "    'i belong to',\n",
       "    'aiml batch',\n",
       "    'aifl batch',\n",
       "    'i am from',\n",
       "    'my pm is',\n",
       "    'blended',\n",
       "    'online',\n",
       "    'i am from',\n",
       "    'hey ya',\n",
       "    'talking to you for first time'],\n",
       "   'responses': ['Hello! how can i help you ?'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Exit',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'cya',\n",
       "    'see you',\n",
       "    'later',\n",
       "    'see you later',\n",
       "    'goodbye',\n",
       "    'i am leaving',\n",
       "    'have a Good day',\n",
       "    'you helped me',\n",
       "    'thanks a lot',\n",
       "    'thanks a ton',\n",
       "    'you are the best',\n",
       "    'great help',\n",
       "    'too good',\n",
       "    'got it',\n",
       "    'i understood',\n",
       "    'cool',\n",
       "    'you are a good learning buddy'],\n",
       "   'responses': ['I hope I was able to assist you, Good Bye'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Olympus',\n",
       "   'patterns': ['olympus',\n",
       "    'explain me how olympus works',\n",
       "    'I am not able to understand olympus',\n",
       "    'olympus window not working',\n",
       "    'no access to olympus',\n",
       "    'unable to see link in olympus',\n",
       "    'no link visible on olympus',\n",
       "    'whom to contact for olympus',\n",
       "    'lot of problem with olympus',\n",
       "    'olypus is not a good tool',\n",
       "    'lot of problems with olympus',\n",
       "    'how to use olympus',\n",
       "    'teach me olympus'],\n",
       "   'responses': ['Link: Olympus wiki : https://olympus.mygreatlearning.com'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'SL',\n",
       "   'patterns': ['i am not able to understand svm',\n",
       "    'explain me how machine learning works',\n",
       "    'i am not able to understand naive bayes',\n",
       "    'i am not able to understand logistic regression',\n",
       "    'i am not able to understand ensemble techb=niques',\n",
       "    'i am not able to understand knn',\n",
       "    'i am not able to understand knn imputer',\n",
       "    'i am not able to understand cross validation',\n",
       "    'i am not able to understand boosting',\n",
       "    'i am not able to understand random forest',\n",
       "    'i am not able to understand ada boosting',\n",
       "    'i am not able to understand gradient boosting',\n",
       "    'machine learning',\n",
       "    'ML',\n",
       "    'SL',\n",
       "    'supervised learning',\n",
       "    'knn',\n",
       "    'logistic regression',\n",
       "    'regression',\n",
       "    'classification',\n",
       "    'naive bayes',\n",
       "    'nb',\n",
       "    'ensemble techniques',\n",
       "    'bagging',\n",
       "    'boosting',\n",
       "    'ada boosting',\n",
       "    'ada',\n",
       "    'gradient boosting',\n",
       "    'hyper parameters'],\n",
       "   'responses': ['Link: Machine Learning wiki : https://en.wikipedia.org/wiki/Machine_learning'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'NN',\n",
       "   'patterns': ['what is deep learning',\n",
       "    'unable to understand deep learning',\n",
       "    'explain me how deep learning works',\n",
       "    'i am not able to understand deep learning',\n",
       "    'not able to understand neural nets',\n",
       "    'very diffult to understand neural nets',\n",
       "    'unable to understand neural nets',\n",
       "    'ann',\n",
       "    'artificial intelligence',\n",
       "    'artificial neural networks',\n",
       "    'weights',\n",
       "    'activation function',\n",
       "    'hidden layers',\n",
       "    'softmax',\n",
       "    'sigmoid',\n",
       "    'relu',\n",
       "    'otimizer',\n",
       "    'forward propagation',\n",
       "    'backward propagation',\n",
       "    'epochs',\n",
       "    'epoch',\n",
       "    'what is an epoch',\n",
       "    'adam',\n",
       "    'sgd'],\n",
       "   'responses': ['Link: Neural Nets wiki: https://en.wikipedia.org/wiki/Neural_network'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Bot',\n",
       "   'patterns': ['what is your name',\n",
       "    'who are you',\n",
       "    'name please',\n",
       "    'when are your hours of opertions',\n",
       "    'what are your working hours',\n",
       "    'hours of operation',\n",
       "    'working hours',\n",
       "    'hours'],\n",
       "   'responses': ['I am your virtual learning assistant eBuddy'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Profane',\n",
       "   'patterns': ['what the hell',\n",
       "    'bloody stupid bot',\n",
       "    'do you think you are very smart',\n",
       "    'screw you',\n",
       "    'i hate you',\n",
       "    'you are stupid',\n",
       "    'go to hell',\n",
       "    'pathetic',\n",
       "    'jerk',\n",
       "    'you are a joke',\n",
       "    'useless piece of shit'],\n",
       "   'responses': ['Please use respectful words'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Ticket',\n",
       "   'patterns': ['my problem is not solved',\n",
       "    'you did not help me',\n",
       "    'not a good solution',\n",
       "    'bad solution',\n",
       "    'not good solution',\n",
       "    'transfer to pm',\n",
       "    'i am not satisfied',\n",
       "    'no help',\n",
       "    'wasted my time',\n",
       "    'useless bot',\n",
       "    'create a ticket'],\n",
       "   'responses': ['Tarnsferring the request to your PM'],\n",
       "   'context_set': ''}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7168",
   "metadata": {},
   "source": [
    "Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830b6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions for tokenization, stemming & creating bag of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "def token_ize(sentence):\n",
    "    # tokenize each sentence by splitting from \" \" \n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def stemming(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def b_o_w(tokenize_sentence,list_of_words):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenize_sentence : [\"hi\",\"how\",\"are\",\"you\"]\n",
    "        result of tokenizer \n",
    "    list_of_words : [\"hi\",\"bye\",\"how\",\"are\",\"you\"]\n",
    "        bag of all words\n",
    "    Returns\n",
    "    -------\n",
    "    list with [1,0,1,1,1]\n",
    "    \"\"\"\n",
    "    stemmed=[stemming(w) for w in tokenize_sentence]\n",
    "    bag=np.zeros(len(list_of_words),dtype=np.float32)\n",
    "    for idx,word in enumerate(list_of_words):\n",
    "        if word in stemmed:\n",
    "            bag[idx]=1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778804b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Words=[] #all words\n",
    "tags=[] #tags\n",
    "x_y=[] #tokenized words with tags\n",
    "\n",
    "for intent in Corpus[\"intents\"]:\n",
    "    tag=intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    for i in intent[\"patterns\"]:\n",
    "        each_word=token_ize(i)\n",
    "        Words.extend(each_word)\n",
    "        x_y.append((each_word,tag))\n",
    "\n",
    "# stem and lower each word\n",
    "ignore_words = ['?', '.', '!']\n",
    "Words = [stemming(w) for w in Words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "Words = sorted(set(Words))\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8b6d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abl',\n",
       " 'access',\n",
       " 'activ',\n",
       " 'ada',\n",
       " 'adam',\n",
       " 'aifl',\n",
       " 'aiml',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ann',\n",
       " 'anyon',\n",
       " 'are',\n",
       " 'artifici',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'batch',\n",
       " 'bay',\n",
       " 'belong',\n",
       " 'best',\n",
       " 'blend',\n",
       " 'bloodi',\n",
       " 'boost',\n",
       " 'bot',\n",
       " 'buddi',\n",
       " 'classif',\n",
       " 'contact',\n",
       " 'cool',\n",
       " 'creat',\n",
       " 'cross',\n",
       " 'cya',\n",
       " 'day',\n",
       " 'deep',\n",
       " 'did',\n",
       " 'diffult',\n",
       " 'do',\n",
       " 'ensembl',\n",
       " 'epoch',\n",
       " 'explain',\n",
       " 'first',\n",
       " 'for',\n",
       " 'forest',\n",
       " 'forward',\n",
       " 'from',\n",
       " 'function',\n",
       " 'go',\n",
       " 'good',\n",
       " 'goodby',\n",
       " 'got',\n",
       " 'gradient',\n",
       " 'great',\n",
       " 'hate',\n",
       " 'have',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hidden',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'hyper',\n",
       " 'i',\n",
       " 'imput',\n",
       " 'in',\n",
       " 'intellig',\n",
       " 'is',\n",
       " 'it',\n",
       " 'jerk',\n",
       " 'joke',\n",
       " 'knn',\n",
       " 'later',\n",
       " 'layer',\n",
       " 'learn',\n",
       " 'learner',\n",
       " 'leav',\n",
       " 'link',\n",
       " 'listen',\n",
       " 'logist',\n",
       " 'lot',\n",
       " 'machin',\n",
       " 'me',\n",
       " 'ml',\n",
       " 'my',\n",
       " 'naiv',\n",
       " 'name',\n",
       " 'nb',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neural',\n",
       " 'no',\n",
       " 'not',\n",
       " 'of',\n",
       " 'olympu',\n",
       " 'olypu',\n",
       " 'on',\n",
       " 'onlin',\n",
       " 'oper',\n",
       " 'opert',\n",
       " 'otim',\n",
       " 'paramet',\n",
       " 'pathet',\n",
       " 'piec',\n",
       " 'pleas',\n",
       " 'pm',\n",
       " 'problem',\n",
       " 'propag',\n",
       " 'random',\n",
       " 'regress',\n",
       " 'relu',\n",
       " 'satisfi',\n",
       " 'screw',\n",
       " 'see',\n",
       " 'sgd',\n",
       " 'shit',\n",
       " 'sigmoid',\n",
       " 'sl',\n",
       " 'smart',\n",
       " 'softmax',\n",
       " 'solut',\n",
       " 'solv',\n",
       " 'stupid',\n",
       " 'supervis',\n",
       " 'svm',\n",
       " 'talk',\n",
       " 'teach',\n",
       " 'techb=niqu',\n",
       " 'techniqu',\n",
       " 'thank',\n",
       " 'the',\n",
       " 'there',\n",
       " 'think',\n",
       " 'ticket',\n",
       " 'time',\n",
       " 'to',\n",
       " 'ton',\n",
       " 'too',\n",
       " 'tool',\n",
       " 'transfer',\n",
       " 'unabl',\n",
       " 'understand',\n",
       " 'understood',\n",
       " 'up',\n",
       " 'use',\n",
       " 'useless',\n",
       " 'valid',\n",
       " 'veri',\n",
       " 'visibl',\n",
       " 'wast',\n",
       " 'weight',\n",
       " 'what',\n",
       " 'when',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'window',\n",
       " 'with',\n",
       " 'work',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'you',\n",
       " 'your']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92b9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bot', 'Exit', 'Intro', 'NN', 'Olympus', 'Profane', 'SL', 'Ticket']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d7dbfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['hi'], 'Intro'),\n",
       " (['how', 'are', 'you'], 'Intro'),\n",
       " (['is', 'anyone', 'there'], 'Intro'),\n",
       " (['hello'], 'Intro'),\n",
       " (['whats', 'up'], 'Intro'),\n",
       " (['hey'], 'Intro'),\n",
       " (['yo'], 'Intro'),\n",
       " (['listen'], 'Intro'),\n",
       " (['please', 'help', 'me'], 'Intro'),\n",
       " (['i', 'am', 'learner', 'from'], 'Intro'),\n",
       " (['i', 'belong', 'to'], 'Intro'),\n",
       " (['aiml', 'batch'], 'Intro'),\n",
       " (['aifl', 'batch'], 'Intro'),\n",
       " (['i', 'am', 'from'], 'Intro'),\n",
       " (['my', 'pm', 'is'], 'Intro'),\n",
       " (['blended'], 'Intro'),\n",
       " (['online'], 'Intro'),\n",
       " (['i', 'am', 'from'], 'Intro'),\n",
       " (['hey', 'ya'], 'Intro'),\n",
       " (['talking', 'to', 'you', 'for', 'first', 'time'], 'Intro'),\n",
       " (['thank', 'you'], 'Exit'),\n",
       " (['thanks'], 'Exit'),\n",
       " (['cya'], 'Exit'),\n",
       " (['see', 'you'], 'Exit'),\n",
       " (['later'], 'Exit'),\n",
       " (['see', 'you', 'later'], 'Exit'),\n",
       " (['goodbye'], 'Exit'),\n",
       " (['i', 'am', 'leaving'], 'Exit'),\n",
       " (['have', 'a', 'Good', 'day'], 'Exit'),\n",
       " (['you', 'helped', 'me'], 'Exit'),\n",
       " (['thanks', 'a', 'lot'], 'Exit'),\n",
       " (['thanks', 'a', 'ton'], 'Exit'),\n",
       " (['you', 'are', 'the', 'best'], 'Exit'),\n",
       " (['great', 'help'], 'Exit'),\n",
       " (['too', 'good'], 'Exit'),\n",
       " (['got', 'it'], 'Exit'),\n",
       " (['i', 'understood'], 'Exit'),\n",
       " (['cool'], 'Exit'),\n",
       " (['you', 'are', 'a', 'good', 'learning', 'buddy'], 'Exit'),\n",
       " (['olympus'], 'Olympus'),\n",
       " (['explain', 'me', 'how', 'olympus', 'works'], 'Olympus'),\n",
       " (['I', 'am', 'not', 'able', 'to', 'understand', 'olympus'], 'Olympus'),\n",
       " (['olympus', 'window', 'not', 'working'], 'Olympus'),\n",
       " (['no', 'access', 'to', 'olympus'], 'Olympus'),\n",
       " (['unable', 'to', 'see', 'link', 'in', 'olympus'], 'Olympus'),\n",
       " (['no', 'link', 'visible', 'on', 'olympus'], 'Olympus'),\n",
       " (['whom', 'to', 'contact', 'for', 'olympus'], 'Olympus'),\n",
       " (['lot', 'of', 'problem', 'with', 'olympus'], 'Olympus'),\n",
       " (['olypus', 'is', 'not', 'a', 'good', 'tool'], 'Olympus'),\n",
       " (['lot', 'of', 'problems', 'with', 'olympus'], 'Olympus'),\n",
       " (['how', 'to', 'use', 'olympus'], 'Olympus'),\n",
       " (['teach', 'me', 'olympus'], 'Olympus'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'svm'], 'SL'),\n",
       " (['explain', 'me', 'how', 'machine', 'learning', 'works'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'naive', 'bayes'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'logistic', 'regression'],\n",
       "  'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'ensemble', 'techb=niques'],\n",
       "  'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'knn'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'knn', 'imputer'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'cross', 'validation'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'boosting'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'random', 'forest'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'ada', 'boosting'], 'SL'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'gradient', 'boosting'],\n",
       "  'SL'),\n",
       " (['machine', 'learning'], 'SL'),\n",
       " (['ML'], 'SL'),\n",
       " (['SL'], 'SL'),\n",
       " (['supervised', 'learning'], 'SL'),\n",
       " (['knn'], 'SL'),\n",
       " (['logistic', 'regression'], 'SL'),\n",
       " (['regression'], 'SL'),\n",
       " (['classification'], 'SL'),\n",
       " (['naive', 'bayes'], 'SL'),\n",
       " (['nb'], 'SL'),\n",
       " (['ensemble', 'techniques'], 'SL'),\n",
       " (['bagging'], 'SL'),\n",
       " (['boosting'], 'SL'),\n",
       " (['ada', 'boosting'], 'SL'),\n",
       " (['ada'], 'SL'),\n",
       " (['gradient', 'boosting'], 'SL'),\n",
       " (['hyper', 'parameters'], 'SL'),\n",
       " (['what', 'is', 'deep', 'learning'], 'NN'),\n",
       " (['unable', 'to', 'understand', 'deep', 'learning'], 'NN'),\n",
       " (['explain', 'me', 'how', 'deep', 'learning', 'works'], 'NN'),\n",
       " (['i', 'am', 'not', 'able', 'to', 'understand', 'deep', 'learning'], 'NN'),\n",
       " (['not', 'able', 'to', 'understand', 'neural', 'nets'], 'NN'),\n",
       " (['very', 'diffult', 'to', 'understand', 'neural', 'nets'], 'NN'),\n",
       " (['unable', 'to', 'understand', 'neural', 'nets'], 'NN'),\n",
       " (['ann'], 'NN'),\n",
       " (['artificial', 'intelligence'], 'NN'),\n",
       " (['artificial', 'neural', 'networks'], 'NN'),\n",
       " (['weights'], 'NN'),\n",
       " (['activation', 'function'], 'NN'),\n",
       " (['hidden', 'layers'], 'NN'),\n",
       " (['softmax'], 'NN'),\n",
       " (['sigmoid'], 'NN'),\n",
       " (['relu'], 'NN'),\n",
       " (['otimizer'], 'NN'),\n",
       " (['forward', 'propagation'], 'NN'),\n",
       " (['backward', 'propagation'], 'NN'),\n",
       " (['epochs'], 'NN'),\n",
       " (['epoch'], 'NN'),\n",
       " (['what', 'is', 'an', 'epoch'], 'NN'),\n",
       " (['adam'], 'NN'),\n",
       " (['sgd'], 'NN'),\n",
       " (['what', 'is', 'your', 'name'], 'Bot'),\n",
       " (['who', 'are', 'you'], 'Bot'),\n",
       " (['name', 'please'], 'Bot'),\n",
       " (['when', 'are', 'your', 'hours', 'of', 'opertions'], 'Bot'),\n",
       " (['what', 'are', 'your', 'working', 'hours'], 'Bot'),\n",
       " (['hours', 'of', 'operation'], 'Bot'),\n",
       " (['working', 'hours'], 'Bot'),\n",
       " (['hours'], 'Bot'),\n",
       " (['what', 'the', 'hell'], 'Profane'),\n",
       " (['bloody', 'stupid', 'bot'], 'Profane'),\n",
       " (['do', 'you', 'think', 'you', 'are', 'very', 'smart'], 'Profane'),\n",
       " (['screw', 'you'], 'Profane'),\n",
       " (['i', 'hate', 'you'], 'Profane'),\n",
       " (['you', 'are', 'stupid'], 'Profane'),\n",
       " (['go', 'to', 'hell'], 'Profane'),\n",
       " (['pathetic'], 'Profane'),\n",
       " (['jerk'], 'Profane'),\n",
       " (['you', 'are', 'a', 'joke'], 'Profane'),\n",
       " (['useless', 'piece', 'of', 'shit'], 'Profane'),\n",
       " (['my', 'problem', 'is', 'not', 'solved'], 'Ticket'),\n",
       " (['you', 'did', 'not', 'help', 'me'], 'Ticket'),\n",
       " (['not', 'a', 'good', 'solution'], 'Ticket'),\n",
       " (['bad', 'solution'], 'Ticket'),\n",
       " (['not', 'good', 'solution'], 'Ticket'),\n",
       " (['transfer', 'to', 'pm'], 'Ticket'),\n",
       " (['i', 'am', 'not', 'satisfied'], 'Ticket'),\n",
       " (['no', 'help'], 'Ticket'),\n",
       " (['wasted', 'my', 'time'], 'Ticket'),\n",
       " (['useless', 'bot'], 'Ticket'),\n",
       " (['create', 'a', 'ticket'], 'Ticket')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9abe3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in x_y:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = b_o_w(pattern_sentence, Words)\n",
    "    X_train.append(bag)\n",
    "    # y: Label index\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f02ff974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b5be271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ee1319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef0350b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135, 162), (135,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "660312d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0096866",
   "metadata": {},
   "source": [
    "Now we will apply classifier models(Neural Network) for classification of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab3fa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "159ceaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d49bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_dim=len(X_train[0])))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "235a68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb734d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 162)              648       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                10432     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,552\n",
      "Trainable params: 13,164\n",
      "Non-trainable params: 388\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10fe8500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "9/9 [==============================] - 1s 3ms/step - loss: 2.6651 - accuracy: 0.1111\n",
      "Epoch 2/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.6401 - accuracy: 0.1259\n",
      "Epoch 3/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.6771 - accuracy: 0.0889\n",
      "Epoch 4/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.5382 - accuracy: 0.1259\n",
      "Epoch 5/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.4780 - accuracy: 0.1556\n",
      "Epoch 6/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.2910 - accuracy: 0.1259\n",
      "Epoch 7/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.2270 - accuracy: 0.1926\n",
      "Epoch 8/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.2280 - accuracy: 0.2667\n",
      "Epoch 9/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1117 - accuracy: 0.2222\n",
      "Epoch 10/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.1988 - accuracy: 0.1852\n",
      "Epoch 11/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.0988 - accuracy: 0.2000\n",
      "Epoch 12/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.9606 - accuracy: 0.2889\n",
      "Epoch 13/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1033 - accuracy: 0.2444\n",
      "Epoch 14/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8912 - accuracy: 0.3407\n",
      "Epoch 15/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8875 - accuracy: 0.3333\n",
      "Epoch 16/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8548 - accuracy: 0.3333\n",
      "Epoch 17/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.7595 - accuracy: 0.3926\n",
      "Epoch 18/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8939 - accuracy: 0.2593\n",
      "Epoch 19/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8209 - accuracy: 0.3037\n",
      "Epoch 20/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.8961 - accuracy: 0.3407\n",
      "Epoch 21/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6770 - accuracy: 0.3556\n",
      "Epoch 22/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6574 - accuracy: 0.3704\n",
      "Epoch 23/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6791 - accuracy: 0.4000\n",
      "Epoch 24/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6406 - accuracy: 0.4222\n",
      "Epoch 25/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.7100 - accuracy: 0.3407\n",
      "Epoch 26/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6368 - accuracy: 0.3852\n",
      "Epoch 27/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.5639 - accuracy: 0.4667\n",
      "Epoch 28/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6777 - accuracy: 0.4296\n",
      "Epoch 29/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.5276 - accuracy: 0.4667\n",
      "Epoch 30/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.6763 - accuracy: 0.4000\n",
      "Epoch 31/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.5910 - accuracy: 0.4444\n",
      "Epoch 32/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4774 - accuracy: 0.5037\n",
      "Epoch 33/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.5674 - accuracy: 0.4074\n",
      "Epoch 34/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4461 - accuracy: 0.4963\n",
      "Epoch 35/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4890 - accuracy: 0.4519\n",
      "Epoch 36/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4607 - accuracy: 0.4815\n",
      "Epoch 37/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4523 - accuracy: 0.5333\n",
      "Epoch 38/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4736 - accuracy: 0.4963\n",
      "Epoch 39/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.3700 - accuracy: 0.5333\n",
      "Epoch 40/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.5086 - accuracy: 0.4444\n",
      "Epoch 41/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.5433 - accuracy: 0.4667\n",
      "Epoch 42/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4283 - accuracy: 0.4741\n",
      "Epoch 43/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2553 - accuracy: 0.6000\n",
      "Epoch 44/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4088 - accuracy: 0.4815\n",
      "Epoch 45/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.4055 - accuracy: 0.5037\n",
      "Epoch 46/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2576 - accuracy: 0.5185\n",
      "Epoch 47/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2834 - accuracy: 0.6000\n",
      "Epoch 48/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2963 - accuracy: 0.5704\n",
      "Epoch 49/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2640 - accuracy: 0.5481\n",
      "Epoch 50/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2321 - accuracy: 0.5630\n",
      "Epoch 51/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.3708 - accuracy: 0.4593\n",
      "Epoch 52/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2392 - accuracy: 0.6000\n",
      "Epoch 53/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.3078 - accuracy: 0.5778\n",
      "Epoch 54/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2166 - accuracy: 0.5704\n",
      "Epoch 55/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2416 - accuracy: 0.5704\n",
      "Epoch 56/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1789 - accuracy: 0.6148\n",
      "Epoch 57/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0536 - accuracy: 0.6444\n",
      "Epoch 58/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1598 - accuracy: 0.6296\n",
      "Epoch 59/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1696 - accuracy: 0.5852\n",
      "Epoch 60/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0459 - accuracy: 0.6667\n",
      "Epoch 61/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1536 - accuracy: 0.5704\n",
      "Epoch 62/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1366 - accuracy: 0.6074\n",
      "Epoch 63/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1057 - accuracy: 0.6296\n",
      "Epoch 64/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1466 - accuracy: 0.5852\n",
      "Epoch 65/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0739 - accuracy: 0.6593\n",
      "Epoch 66/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0341 - accuracy: 0.6889\n",
      "Epoch 67/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0267 - accuracy: 0.6593\n",
      "Epoch 68/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1018 - accuracy: 0.6370\n",
      "Epoch 69/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1136 - accuracy: 0.6370\n",
      "Epoch 70/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0305 - accuracy: 0.6815\n",
      "Epoch 71/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0465 - accuracy: 0.6296\n",
      "Epoch 72/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0204 - accuracy: 0.6667\n",
      "Epoch 73/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.1178 - accuracy: 0.5926\n",
      "Epoch 74/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0435 - accuracy: 0.6741\n",
      "Epoch 75/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0161 - accuracy: 0.6593\n",
      "Epoch 76/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9833 - accuracy: 0.6296\n",
      "Epoch 77/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9629 - accuracy: 0.6593\n",
      "Epoch 78/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8893 - accuracy: 0.7259\n",
      "Epoch 79/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9129 - accuracy: 0.6963\n",
      "Epoch 80/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9452 - accuracy: 0.6667\n",
      "Epoch 81/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8318 - accuracy: 0.7259\n",
      "Epoch 82/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9767 - accuracy: 0.6667\n",
      "Epoch 83/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9535 - accuracy: 0.7185\n",
      "Epoch 84/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8637 - accuracy: 0.6815\n",
      "Epoch 85/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8246 - accuracy: 0.7704\n",
      "Epoch 86/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8076 - accuracy: 0.7407\n",
      "Epoch 87/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8545 - accuracy: 0.7333\n",
      "Epoch 88/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7988 - accuracy: 0.7259\n",
      "Epoch 89/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9379 - accuracy: 0.6889\n",
      "Epoch 90/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7671 - accuracy: 0.7630\n",
      "Epoch 91/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8069 - accuracy: 0.7852\n",
      "Epoch 92/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8885 - accuracy: 0.7111\n",
      "Epoch 93/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8170 - accuracy: 0.7704\n",
      "Epoch 94/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.9679 - accuracy: 0.6519\n",
      "Epoch 95/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7940 - accuracy: 0.7481\n",
      "Epoch 96/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.8296\n",
      "Epoch 97/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8370 - accuracy: 0.6963\n",
      "Epoch 98/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8530 - accuracy: 0.7111\n",
      "Epoch 99/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7471 - accuracy: 0.7852\n",
      "Epoch 100/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7435 - accuracy: 0.7630\n",
      "Epoch 101/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8124 - accuracy: 0.7259\n",
      "Epoch 102/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8864 - accuracy: 0.7111\n",
      "Epoch 103/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8232 - accuracy: 0.7111\n",
      "Epoch 104/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7124 - accuracy: 0.7704\n",
      "Epoch 105/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7904 - accuracy: 0.7630\n",
      "Epoch 106/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7332 - accuracy: 0.7778\n",
      "Epoch 107/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7409 - accuracy: 0.7630\n",
      "Epoch 108/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.8000\n",
      "Epoch 109/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7775 - accuracy: 0.7185\n",
      "Epoch 110/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.8065 - accuracy: 0.6963\n",
      "Epoch 111/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.7704\n",
      "Epoch 112/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7442 - accuracy: 0.7778\n",
      "Epoch 113/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.8148\n",
      "Epoch 114/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.7926\n",
      "Epoch 115/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7291 - accuracy: 0.7630\n",
      "Epoch 116/150\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.7116 - accuracy: 0.7852\n",
      "Epoch 117/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.8222\n",
      "Epoch 118/150\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7766 - accuracy: 0.7259\n",
      "Epoch 119/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.8074\n",
      "Epoch 120/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.8000\n",
      "Epoch 121/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7206 - accuracy: 0.7481\n",
      "Epoch 122/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.8000\n",
      "Epoch 123/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.8222\n",
      "Epoch 124/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6966 - accuracy: 0.7630\n",
      "Epoch 125/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.7852\n",
      "Epoch 126/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.8593\n",
      "Epoch 127/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.8222\n",
      "Epoch 128/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.7778\n",
      "Epoch 129/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.8148\n",
      "Epoch 130/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.8148\n",
      "Epoch 131/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.8519\n",
      "Epoch 132/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.8222\n",
      "Epoch 133/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.8222\n",
      "Epoch 134/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5571 - accuracy: 0.8074\n",
      "Epoch 135/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.8370\n",
      "Epoch 136/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.8222\n",
      "Epoch 137/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.8370\n",
      "Epoch 138/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.8963\n",
      "Epoch 139/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8593\n",
      "Epoch 140/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.8370\n",
      "Epoch 141/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.7926\n",
      "Epoch 142/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.8148\n",
      "Epoch 143/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.7926\n",
      "Epoch 144/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.8519\n",
      "Epoch 145/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.8444\n",
      "Epoch 146/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.8370\n",
      "Epoch 147/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.8370\n",
      "Epoch 148/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.8667\n",
      "Epoch 149/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5372 - accuracy: 0.8296\n",
      "Epoch 150/150\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.8370\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model1=model.fit(X_train, y_train, epochs=150, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04394c6c",
   "metadata": {},
   "source": [
    "We trained the model with available Coprus & got accuracy of 83.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc158a",
   "metadata": {},
   "source": [
    "The model accuracy can be improved by tuning the neural network & by improving the corpus of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b648643",
   "metadata": {},
   "source": [
    "Now, preparing ChatBot with prediction of input sentances by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12177543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    bot_name = \"eBuddy\"\n",
    "    print(\"Let's chat! (type 'quit' to exit)\")\n",
    "    print('if answer is not right(type:*)')\n",
    "    while True:\n",
    "        sentence = input(\"You: \") #input from user\n",
    "        if sentence.lower()==\"*\":\n",
    "            print(f'{bot_name}: Please rephrase your question & try again')\n",
    "            sentence = input(\"You: \")\n",
    "        if sentence == \"quit\":\n",
    "            break\n",
    "            \n",
    "        sentence = token_ize(sentence) #tokenize the input\n",
    "        X = b_o_w(sentence, Words) #creating bag of words\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        y_pred = model.predict(X) #prediction with above trained NN model\n",
    "        y_pred_index=np.argmax(y_pred)\n",
    "        tag = tags[y_pred_index] #predicted tag of input\n",
    "\n",
    "        for intent in Corpus['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d79c414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "if answer is not right(type:*)\n",
      "You: hi\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "eBuddy: Hello! how can i help you ?\n",
      "You: what is your name\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "eBuddy: I am your virtual learning assistant eBuddy\n",
      "You: what is machine learning\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "eBuddy: Link: Machine Learning wiki : https://en.wikipedia.org/wiki/Machine_learning\n",
      "You: what is neual network\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "eBuddy: Link: Neural Nets wiki: https://en.wikipedia.org/wiki/Neural_network\n",
      "You: how to go to olympus\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "eBuddy: Link: Olympus wiki : https://olympus.mygreatlearning.com\n",
      "You: *\n",
      "eBuddy: Please rephrase your question & try again\n",
      "You: you are bad\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "eBuddy: Tarnsferring the request to your PM\n",
      "You: please transfer to PM\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "eBuddy: Tarnsferring the request to your PM\n",
      "You: Thanks\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "eBuddy: I hope I was able to assist you, Good Bye\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
